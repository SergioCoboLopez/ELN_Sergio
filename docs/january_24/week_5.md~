---
layout: default
title: Week 5
parent: January 2024
nav_order: 4
---

| Goal | Notes |
| ----------- | ----------- |                               
|(W)|RyC,Tipping points and BP |
|(PH)| |
|(R)| Tipping elements |
|(Code)|Upper bound of the error |
|(O)| |

## January 29

**W** RyC

I am realizing that the nestedness part of the paper of the enterotypes is really important:

"Our results suggest that the taxonomic composition of healthy human gut microbiomes follow a nested structure, which are not only similar to those found in other ecological networks
throughout nature, but also predictive of unobserved abundances."

I don't know how to explain this. Maybe I am putting myself in trouble with the question that I pose. At the same time, I should not make things more complicated for myself.

How are the models in GoldSim different from python?
In reality, they are the same, but there is a different narrative: GoldSim was more applied and specific, Python was more general.

What is a tipping point? How would you define it?

It is a point upon which the behavior of the system changes.

Tipping points are critical thresholds.

## January 30

**R** R's paper

Artificial Neural Networks can accurately approximate arbitrary functions (that is expressiveness)
Universal approximation theorem.

The problem is that the models they produce are extremely complex (they can have trillions of parameters)

Reverse of the universal approximation theorem. The space of models that can be generated by ANNs being generated by closed-form mathematical functions.

I think he generates data with ANNs and then tries to recover that with the BMS?
So the ANN generates a function (whatever it is) and then you plot that function on a 5x5 grid?

whether the space of models that can be generated by ANNs can also be generated using closed-form mathematical
functions

whether it is possible to get good approximations to ANN models using **simple** closed-form mathematical expressions

Approximations involving 10s of elementary functions and 10 parameters can approximate feed-forward networks with 100s of parameters.