---
layout: default
title: Week 2
parent: February 2024
nav_order: 2
---


| Goal | Notes |
| ----------- | ----------- |
|(W)|BP - state of the art and rest of the form|
|(PH)| |
|(R)|Tipping elements paper |
|(Code)|ANN. Produce 1d result, Start Implementing Relative error and log-ratio |
|(O)|Finish BP paperwork |


## February 12

**code** ANN 1d

My plan was to propose a plan to R.
1. Generate 1d data with different characteristics: different number of hidden cells.
2. Recover that data with the BMS
3. Look at the results.


**W** BP
I should put into writing the things I've learned on Saturday about climate tipping elements.

Lenton 2008 does say (in one way or another) that the temporal scale and the abruptness are not that important.

Ok, so how do I go on now? The distinction of Kopp is based on the "abruptness" of the tipping point.

I don't think it's important to provide the new definition of tipping element here or redefine the list.

So I have a definition now. The next thing would be to stress how and why tipping elements are important. And then I should explain what has been done in terms of tipping elements.

What you want to do now is to state that climate change is already going on. And in that context, you do want to be aware of tipping elements, because they can really fuck you up (why and how?)

However, predicting tipping points is not easy.

And with that you can move towards predicting tipping points and not making your life more complicated. I think it is just a matter of going to the methods at this point. That makes things easier for me. Although that leaves the specific tipping elements a bit astray.


**R** Science Advances AMOC
They use a previously existing model that can incorporate different elements and segments of the Earth, apparently. The particular thing about their model is that they introduce fresh water, I think. 

"After model year 800, a clear negative trend appears because of the increasing freshwater forcing.". AMOC stops after 1750 years.

GCM - Global Climate Model

**code**

Doing the 1d version of the code is not as simple as I had thought. There is a loop that I don't understand here.

There are three parameters (at least) to play with:

Input layer size: how many data elements you want to train the neural network with
Number of (hidden) layers - NL: I suppose this will train the model more, the more layers there are.
Layer size - LS: The same as above?


So ILS would be a parameter of the type of problem I want to solve.

The output layer number, corresponds to the number of things you want to predict.

"a good rule of thumb is that there should not be a radical difference between the number of nodes in your input layer and the number of nodes in your hidden layer." That would refer to the layer size.

He's already increasing the Input Layer Size on every step.

You probably want to have a higher number of parameters on the ANN than the number of parameters/variables of the BMS.

So maybe you can do a BMS from an expression and not from data. Maybe that's what the tree does.

So you generate data with the ANN.

And then you try to replicate the data with the BMS.

Then, you get the "best" function of a BMS run. Which run you take?
There is a parameter called 'DATAID' that makes something I don't understand.
You could also play with the prior? Although you probably want to have a prior with small number of parameters. At the same time, it seems you are "importing" a file with a set number of parameters.

There should not be much else to change once you can really edit and write the codes.

Problems:
 I do not have permissions for certain things in the notebooks.
 Should I code in local files or notebooks? Does the lab have a preference?
 

copy-paste what you need from the folder
look at the 'sample' scripts

Remember that you can save scripts as python files. Remember you can adapt the size of the tree.

This code is very well-written and I should note how things are ellegantly coded.
For some reason, the functions generated look like sigmoids.

Ok, so I have the data generated by the ANNs now.

I should run the BMS to produce functions.
Remember that traces are posteriors (somehow)
Probably traces are your output files?

Now I need to understand what needs to be done to choose the sample.py script that I should use.
I need to run the data (that I've just generated) into the BMS.

The numbers in "sample_single_x" refer to the number of dimensions

pt is some kind of parameter of the BMS.

Ok, so let's try this. We will make a BMS run. 'dataid' is going to be 0. And 'runid' is going to be 0 too.

BMS runs take a much longer time than I had expected.


**O** I don't think what I am doing makes sense from a practical point of view.


## February 13

**code**
The code for the first trace file is still working. And this is weird. Is there something wrong?
What I am seeing is that csv file is much heavier than the corresponding file for two dimensional systems (~16 to ~1 mb)

What could be going on?
One possibility is that the BMS is having a hard time finding a sufficiently low MDL. It might be the nature of that system. This could be consistent with the very high mdl that we're seeing.
Our system is right now at -39.15 after almost 34000 steps.
So arriving at 50000 steps can happen, apparently.

I think I could try a sample_single instance, but would not necessarily make my life easier. How do the files look?


1.1. -        4288  -758.875934510267
1.1.single -  49999 -1650.49164662031

1.2. -        2925   -615.841806200319
1.2.single -  9398   -1230.7773345588

2.1. -       
2.1.single - 49999   -1548.1312547594

2.2. -
2.2.single -

Actually, all of them seem to be single rather than not single. And the single ones I have explored are the ones arriving at 50000 steps.

I will try to code the "single" version of the sample code.

**W** BP

For the impact section I wrote this placeholders
1. Having my own research line
2. Leading innovative projects
3. Scientific publications
4. Mentoring - not sure how to write it
5. International collaborations - not sure how to write it
6. Bringing science closer to society - not sure how to write it

**code**

Is the relative error what I think it is?

abs(v - vapprox/v) v is theoretical value. vapprox is the predicted value by the model

we have abs(bt - bm/bt), with 'bt' representing the theoretical value, and bm the model value.

Several options to represent the error:
1. A curve with the relative error of bacteria, phage, and mean error of both.
2. The integrated relative error of bacteria and phage (with error)
3. The average integrated relative erro of phage and bacteria. I have two errors there.

There is a small hop in the relative error. That hope comes from how I calculated the simplified model.
Every time I change the dynamic regime I solve the equation for the new regime with the initial conditions being the last timestep in the previous regime and the final value of the previous regime. So there is a tiny bit of overlap that you are not going to see.
The first value on the actual new dynamic regime is the solution of a new equation, hence the hop.
This is formally correct.

## February 16

**code** You get an extra point per each dynamic regime. Is there a quick fix for this?
This is weird. Sometimes the simplified solution is shorter thant the "normal" one.

There is a small error with the simplified models. I thought simplified dynamics would have more points simply because of the way the code is built. But this is not always true, and happens the other way around for the decay.