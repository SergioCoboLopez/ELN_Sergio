--
layout: default
title: Week 2
parent: February 2024
nav_order: 2
---


| Goal | Notes |
| ----------- | ----------- |
|(W)|BP - state of the art and rest of the form|
|(PH)| |
|(R)|Tipping elements paper |
|(Code)|ANN. Produce 1d result, Start Implementing Relative error and log-ratio |
|(O)|Finish BP paperwork |


## February 12

**code** ANN 1d

My plan was to propose a plan to R.
1. Generate 1d data with different characteristics: different number of hidden cells.
2. Recover that data with the BMS
3. Look at the results.


**W** BP
I should put into writing the things I've learned on Saturday about climate tipping elements.

Lenton 2008 does say (in one way or another) that the temporal scale and the abruptness are not that important.

Ok, so how do I go on now? The distinction of Kopp is based on the "abruptness" of the tipping point.

I don't think it's important to provide the new definition of tipping element here or redefine the list.

So I have a definition now. The next thing would be to stress how and why tipping elements are important. And then I should explain what has been done in terms of tipping elements.

What you want to do now is to state that climate change is already going on. And in that context, you do want to be aware of tipping elements, because they can really fuck you up (why and how?)

However, predicting tipping points is not easy.

And with that you can move towards predicting tipping points and not making your life more complicated. I think it is just a matter of going to the methods at this point. That makes things easier for me. Although that leaves the specific tipping elements a bit astray.


**R** Science Advances AMOC
They use a previously existing model that can incorporate different elements and segments of the Earth, apparently. The particular thing about their model is that they introduce fresh water, I think. 

"After model year 800, a clear negative trend appears because of the increasing freshwater forcing.". AMOC stops after 1750 years.

GCM - Global Climate Model

**code**

Doing the 1d version of the code is not as simple as I had thought. There is a loop that I don't understand here.

There are three parameters (at least) to play with:

Input layer size: how many data elements you want to train the neural network with
Number of (hidden) layers - NL: I suppose this will train the model more, the more layers there are.
Layer size - LS: The same as above?


So ILS would be a parameter of the type of problem I want to solve.

The output layer number, corresponds to the number of things you want to predict.

"a good rule of thumb is that there should not be a radical difference between the number of nodes in your input layer and the number of nodes in your hidden layer." That would refer to the layer size.

He's already increasing the Input Layer Size on every step.

You probably want to have a higher number of parameters on the ANN than the number of parameters/variables of the BMS.

So maybe you can do a BMS from an expression and not from data. Maybe that's what the tree does.

So you generate data with the ANN.

And then you try to replicate the data with the BMS.

Then, you get the "best" function of a BMS run. Which run you take?
There is a parameter called 'DATAID' that makes something I don't understand.
You could also play with the prior? Although you probably want to have a prior with small number of parameters. At the same time, it seems you are "importing" a file with a set number of parameters.

There should not be much else to change once you can really edit and write the codes.

Problems:
 I do not have permissions for certain things in the notebooks.
 Should I code in local files or notebooks? Does the lab have a preference?
 

copy-paste what you need from the folder
look at the 'sample' scripts

Remember that you can save scripts as python files. Remember you can adapt the size of the tree.

This code is very well-written and I should note how things are ellegantly coded.
For some reason, the functions generated look like sigmoids.

Ok, so I have the data generated by the ANNs now.

I should run the BMS to produce functions.
Remember that traces are posteriors (somehow)
Probably traces are your output files?

Now I need to understand what needs to be done to choose the sample.py script that I should use.
I need to run the data (that I've just generated) into the BMS.

The numbers in "sample_single_x" refer to the number of dimensions

pt is some kind of parameter of the BMS.

Ok, so let's try this. We will make a BMS run. 'dataid' is going to be 0. And 'runid' is going to be 0 too.

BMS runs take a much longer time than I had expected.


**O** I don't think what I am doing makes sense from a practical point of view.